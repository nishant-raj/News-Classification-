{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'Desktop'\n",
      "C:\\Users\\GC Lab\\Desktop\n"
     ]
    }
   ],
   "source": [
    "cd Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1  She left her husband. He killed their children...  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset\n",
    "\n",
    "import pandas as pd\n",
    "data=pd.read_json(\"News_Category_Dataset.json\",lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge headline column and short_description to get one extra feature named important feature\n",
    "\n",
    "data['imp'] = data['short_description'].astype(str) + data['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08871b9d4ae455ab9f0b0be678d0d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#import required library \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = set(stopwords.words('english'))\n",
    "from string import punctuation\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124988, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataset\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124765, 7)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates and null value from imp column\n",
    "\n",
    "data = data.drop_duplicates('imp')\n",
    "data = data[~data['imp'].isnull()]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668\n"
     ]
    }
   ],
   "source": [
    "# these are most frequent stop words here used my own generated stopwords rather than predefied NLP having very less no. of stop words approx 150\n",
    "# stopwords.txt is imported drom my local system rather than python pre-defined \n",
    "\n",
    "stop_words = []\n",
    "f = open('stopwords.txt', 'r')\n",
    "for l in f.readlines():\n",
    "    stop_words.append(l.replace('\\n', ''))\n",
    "additional_stop_words = ['t', 'will']\n",
    "stop_words += additional_stop_words\n",
    "\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'able', 'about', 'above', 'abst', 'accordance', 'according', 'accordingly', 'across', 'act', 'actually', 'added', 'adj', 'affected', 'affecting', 'affects', 'after', 'afterwards', 'again', 'against', 'ah', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'announce', 'another', 'any', 'anybody', 'anyhow', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apparently', 'approximately', 'are', 'aren', 'arent', 'arise', 'around', 'as', 'aside', 'ask', 'asking', 'at', 'auth', 'available', 'away', 'awfully', 'b', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'beginning', 'beginnings', 'begins', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'between', 'beyond', 'biol', 'both', 'brief', 'briefly', 'but', 'by', 'c', 'ca', 'came', 'can', 'cannot', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'co', 'com', 'come', 'comes', 'contain', 'containing', 'contains', 'could', 'couldnt', 'd', 'date', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', 'done', \"don't\", 'down', 'downwards', 'due', 'during', 'e', 'each', 'ed', 'edu', 'effect', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'especially', 'et', 'et-al', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'except', 'f', 'far', 'few', 'ff', 'fifth', 'first', 'five', 'fix', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'found', 'four', 'from', 'further', 'furthermore', 'g', 'gave', 'get', 'gets', 'getting', 'give', 'given', 'gives', 'giving', 'go', 'goes', 'gone', 'got', 'gotten', 'h', 'had', 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', 'hed', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'heres', 'hereupon', 'hers', 'herself', 'hes', 'hi', 'hid', 'him', 'himself', 'his', 'hither', 'home', 'how', 'howbeit', 'however', 'hundred', 'i', 'id', 'ie', 'if', \"i'll\", 'im', 'immediate', 'immediately', 'importance', 'important', 'in', 'inc', 'indeed', 'index', 'information', 'instead', 'into', 'invention', 'inward', 'is', \"isn't\", 'it', 'itd', \"it'll\", 'its', 'itself', \"i've\", 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'kg', 'km', 'know', 'known', 'knows', 'l', 'largely', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'lets', 'like', 'liked', 'likely', 'line', 'little', \"'ll\", 'look', 'looking', 'looks', 'ltd', 'm', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', 'me', 'mean', 'means', 'meantime', 'meanwhile', 'merely', 'mg', 'might', 'million', 'miss', 'ml', 'more', 'moreover', 'most', 'mostly', 'mr', 'mrs', 'much', 'mug', 'must', 'my', 'myself', 'n', 'na', 'name', 'namely', 'nay', 'nd', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'nor', 'normally', 'nos', 'not', 'noted', 'nothing', 'now', 'nowhere', 'o', 'obtain', 'obtained', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'omitted', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'ord', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'owing', 'own', 'p', 'page', 'pages', 'part', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus', 'poorly', 'possible', 'possibly', 'potentially', 'pp', 'predominantly', 'present', 'previously', 'primarily', 'probably', 'promptly', 'proud', 'provides', 'put', 'q', 'que', 'quickly', 'quite', 'qv', 'r', 'ran', 'rather', 'rd', 're', 'readily', 'really', 'recent', 'recently', 'ref', 'refs', 'regarding', 'regardless', 'regards', 'related', 'relatively', 'research', 'respectively', 'resulted', 'resulting', 'results', 'right', 'run', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'sec', 'section', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sent', 'seven', 'several', 'shall', 'she', 'shed', \"she'll\", 'shes', 'should', \"shouldn't\", 'show', 'showed', 'shown', 'showns', 'shows', 'significant', 'significantly', 'similar', 'similarly', 'since', 'six', 'slightly', 'so', 'some', 'somebody', 'somehow', 'someone', 'somethan', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specifically', 'specified', 'specify', 'specifying', 'still', 'stop', 'strongly', 'sub', 'substantially', 'successfully', 'such', 'sufficiently', 'suggest', 'sup', 'sure', 'take', 'taken', 'taking', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that've\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'thered', 'therefore', 'therein', \"there'll\", 'thereof', 'therere', 'theres', 'thereto', 'thereupon', \"there've\", 'these', 'they', 'theyd', \"they'll\", 'theyre', \"they've\", 'think', 'this', 'those', 'thou', 'though', 'thoughh', 'thousand', 'throug', 'through', 'throughout', 'thru', 'thus', 'til', 'tip', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'ts', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'ups', 'us', 'use', 'used', 'useful', 'usefully', 'usefulness', 'uses', 'using', 'usually', 'v', 'value', 'various', \"'ve\", 'very', 'via', 'viz', 'vol', 'vols', 'vs', 'w', 'want', 'wants', 'was', 'wasnt', 'way', 'we', 'wed', 'welcome', \"we'll\", 'went', 'were', 'werent', \"we've\", 'what', 'whatever', \"what'll\", 'whats', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'wheres', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whim', 'whither', 'who', 'whod', 'whoever', 'whole', \"who'll\", 'whom', 'whomever', 'whos', 'whose', 'why', 'widely', 'willing', 'wish', 'with', 'within', 'without', 'wont', 'words', 'world', 'would', 'wouldnt', 'www', 'x', 'y', 'yes', 'yet', 'you', 'youd', \"you'll\", 'your', 'youre', 'yours', 'yourself', 'yourselves', \"you've\", 'z', 'zero', 't', 'will']\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# these are stopwords and punctuation which is useless in our corpus so we should remove this\n",
    "\n",
    "print(list(stop_words))\n",
    "print(list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the imp column of  text by changing into simple words free from non-ascii converted into lower case \n",
    "\n",
    "from functools import reduce\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the whole lines into tockens\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = clean_text(text)    \n",
    "    tokens = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    tokens = (reduce(lambda x,y: x+y, tokens,[]))\n",
    "    tockens = tokens[:] \n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "     \n",
    "    \n",
    "    \n",
    "    tokens = list(filter(lambda token: token not in (list(stop_words) + list(punctuation)) , tokens))\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdb18ddbaee4b30963c25a65d6e5cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=124765), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#tockanize words of short_description free from stopwords punctuation \n",
    "\n",
    "data['tokens'] = data['imp'].progress_map(lambda d: tokenizer(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "important feature:  She left her husband. He killed their children. Just another day in America.There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV\n",
      "\n",
      "tokens:  ['left', 'husband', 'killed', 'children', 'day', 'america', 'mass', 'shootings', 'texas', 'week', 'tv']\n",
      "\n",
      "category:  CRIME\n",
      "\n",
      "important feature:  The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.Hugh Grant Marries For The First Time At Age 57\n",
      "\n",
      "tokens:  ['actor', 'longtime', 'girlfriend', 'anna', 'eberstein', 'tied', 'knot', 'civil', 'ceremony', 'hugh', 'grant', 'marries', 'time', 'age']\n",
      "\n",
      "category:  ENTERTAINMENT\n",
      "\n",
      "important feature:  The actor gives Dems an ass-kicking for not fighting hard enough against Donald Trump.Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\n",
      "\n",
      "tokens:  ['actor', 'dems', 'ass', 'kicking', 'fighting', 'hard', 'donald', 'trump', 'jim', 'carrey', 'blasts', 'castrato', 'adam', 'schiff', 'democrats', 'artwork']\n",
      "\n",
      "category:  ENTERTAINMENT\n",
      "\n",
      "important feature:  The \"Dietland\" actress said using the bags is a \"really cathartic, therapeutic moment.\"Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog\n",
      "\n",
      "tokens:  ['dietland', 'actress', 'bags', 'cathartic', 'therapeutic', 'moment', 'julianna', 'margulies', 'donald', 'trump', 'poop', 'bags', 'pick', 'dog']\n",
      "\n",
      "category:  ENTERTAINMENT\n",
      "\n",
      "important feature:  \"It is not right to equate horrific incidents of sexual assault with misplaced compliments or humor,\" he said in a statement.Morgan Freeman 'Devastated' That Sexual Harassment Claims Could Undermine Legacy\n",
      "\n",
      "tokens:  ['equate', 'horrific', 'incidents', 'sexual', 'assault', 'misplaced', 'compliments', 'humor', 'statement', 'morgan', 'freeman', 'evastated', 'sexual', 'harassment', 'claims', 'undermine', 'legacy']\n",
      "\n",
      "category:  ENTERTAINMENT\n"
     ]
    }
   ],
   "source": [
    "# import feature column ate converted into tockens free from stopwords \n",
    "\n",
    "for descripition, tokens, category in zip(data['imp'].head(5), data['tokens'].head(5),data['category'].head(5)):\n",
    "    print('\\nimportant feature: ', descripition)\n",
    "    print('\\ntokens: ', tokens)\n",
    "    print('\\ncategory: ',category)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>imp</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zach Carter</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>2016-07-23</td>\n",
       "      <td>Hillary Clinton Does Not Understand Her Own Su...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hillary-c...</td>\n",
       "      <td>She just pointlessly poked them in the eye.</td>\n",
       "      <td>She just pointlessly poked them in the eye.Hil...</td>\n",
       "      <td>[pointlessly, poked, eye, hillary, clinton, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Greenberg, ContributorTravel Editor for ...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-02</td>\n",
       "      <td>Incredible Seaplane Views of Anchorage, Alaska</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hidden-ge...</td>\n",
       "      <td>Head over to Lake Hood, the busiest seaplane p...</td>\n",
       "      <td>Head over to Lake Hood, the busiest seaplane p...</td>\n",
       "      <td>[head, lake, hood, busiest, seaplane, port, ai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             authors  category       date  \\\n",
       "0                                        Zach Carter  POLITICS 2016-07-23   \n",
       "1  Peter Greenberg, ContributorTravel Editor for ...    TRAVEL 2014-05-02   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Hillary Clinton Does Not Understand Her Own Su...   \n",
       "1     Incredible Seaplane Views of Anchorage, Alaska   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/hillary-c...   \n",
       "1  https://www.huffingtonpost.com/entry/hidden-ge...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0        She just pointlessly poked them in the eye.   \n",
       "1  Head over to Lake Hood, the busiest seaplane p...   \n",
       "\n",
       "                                                 imp  \\\n",
       "0  She just pointlessly poked them in the eye.Hil...   \n",
       "1  Head over to Lake Hood, the busiest seaplane p...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [pointlessly, poked, eye, hillary, clinton, un...  \n",
       "1  [head, lake, hood, busiest, seaplane, port, ai...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data2 contains less no. of tuples then in original dataset\n",
    "\n",
    "data2 = data.sample(80000, random_state=42)\n",
    "data2.reset_index(inplace=True, drop=True)\n",
    "data2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf vectorization performed on tockenized dataset \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=5, analyzer='word', ngram_range=(1,2), stop_words=stop_words)\n",
    "vx = vectorizer.fit_transform(list(data['tokens'].map(lambda tokens: ' '.join(tokens))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124765, 43817)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of tf-idf sparse matrix\n",
    "\n",
    "vx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix changed into array\n",
    "\n",
    "dense = vx.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets is splitted into train and test \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "des_tr, des_te, cat_tr, cat_te = train_test_split(dense,data['category'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset is fit on linear support vector\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "model.fit(des_tr,cat_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 61.57\n"
     ]
    }
   ],
   "source": [
    "# prediction is made on test data and accuracy is calculate\n",
    "\n",
    "y_pred = model.predict(des_te)\n",
    "acc = accuracy_score(cat_te, y_pred)\n",
    "print(\"Accuracy {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: SCIENCE            predicted: SCIENCE\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: IMPACT            predicted: RELIGION\n",
      "actual: BLACK VOICES            predicted: WOMEN\n",
      "actual: SCIENCE            predicted: GREEN\n",
      "actual: HEALTHY LIVING            predicted: HEALTHY LIVING\n",
      "actual: IMPACT            predicted: SCIENCE\n",
      "actual: PARENTS            predicted: PARENTS\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: COMEDY            predicted: COMEDY\n",
      "actual: ENTERTAINMENT            predicted: ENTERTAINMENT\n",
      "actual: PARENTS            predicted: PARENTS\n",
      "actual: POLITICS            predicted: GREEN\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: GOOD NEWS            predicted: SPORTS\n",
      "actual: COMEDY            predicted: POLITICS\n",
      "actual: IMPACT            predicted: EDUCATION\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: PARENTS            predicted: EDUCATION\n",
      "actual: PARENTS            predicted: PARENTS\n",
      "actual: SPORTS            predicted: SPORTS\n",
      "actual: TASTE            predicted: TASTE\n",
      "actual: WORLDPOST            predicted: SCIENCE\n",
      "actual: CRIME            predicted: POLITICS\n",
      "actual: COMEDY            predicted: COMEDY\n",
      "actual: ENTERTAINMENT            predicted: ENTERTAINMENT\n",
      "actual: ENTERTAINMENT            predicted: ENTERTAINMENT\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: TRAVEL            predicted: TRAVEL\n",
      "actual: BLACK VOICES            predicted: ENTERTAINMENT\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: WOMEN            predicted: ENTERTAINMENT\n",
      "actual: ENTERTAINMENT            predicted: ENTERTAINMENT\n",
      "actual: IMPACT            predicted: QUEER VOICES\n",
      "actual: IMPACT            predicted: COLLEGE\n",
      "actual: WEIRD NEWS            predicted: WEIRD NEWS\n",
      "actual: WORLDPOST            predicted: TRAVEL\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: WOMEN            predicted: WOMEN\n",
      "actual: HEALTHY LIVING            predicted: HEALTHY LIVING\n",
      "actual: ENTERTAINMENT            predicted: WEIRD NEWS\n",
      "actual: SPORTS            predicted: TASTE\n",
      "actual: THE WORLDPOST            predicted: SPORTS\n",
      "actual: POLITICS            predicted: BLACK VOICES\n",
      "actual: POLITICS            predicted: POLITICS\n",
      "actual: WEIRD NEWS            predicted: ENTERTAINMENT\n"
     ]
    }
   ],
   "source": [
    "# these are 50 sample of actual value and predicted result on test data we can change range to get more sample\n",
    "# these results shows model doesn't predicted weird result even if wrong prediction is somewhat similar to actual \n",
    "\n",
    "p=np.asarray(cat_te)\n",
    "for x in range(5000,5050):\n",
    "    print(\"actual:\" ,p[x] , \"           predicted:\" ,y_pred[x])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear regression model is trained on the dataset and accuracy is calculated\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()\n",
    "model.fit(des_tr,cat_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 59.89\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(des_te)\n",
    "acc = accuracy_score(cat_te, y_pred)\n",
    "print(\"Accuracy {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multinominal naive baise model is fit and prediction is measured  \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "model.fit(des_tr,cat_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 45.57\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(des_te)\n",
    "acc = accuracy_score(cat_te, y_pred)\n",
    "print(\"Accuracy {:.2f}\".format(acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
